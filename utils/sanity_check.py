from mbrl.network import Dynamics
from mbrl.mpc import RandomShooter
from rolls import rollouts
from mbrl.runner import StackStAct
from mbrl.wrapped_env import QuadrotorEnv, QuadrotorAcelRotmat
from utils.gen_trajectories import Trajectory

from utils.analize_dynamics import plot_error_map
from utils.utility import DecodeEnvironment
import numpy as np
import torch


from IPython.core.debugger import set_trace

class SanityCheck:
    """
        Sanity Check class
        ===================
        Provides an interface to check how well the dynamics is predicting
        Given an initial state S0, taken at point "t_init" time-step, predict the
        following "horizon" future timesteps.

        A comparison is provided between the states generated by the artificial dynamics
        and the ground truth dynamics. 

        First, actions are generated by the MPC and dynamics, the resulting true-actions
        and true-states are stored.
        Then, the same actions is taken from the initial state to generate a new set of states
        these new states are stored as artificial-states, the a comparison is performed.

        @Parameters:
        h               :   horizon
        c               :   number of candidates to the MPC
        mpc             :   Model Predictive Controler
        env             :   Environment, 'QuadrotorEnv'
        t_init          :   t_step to take the initial State
        traj            :   Trajectory over will be generated the states
        max_path_length :   The maximum length of a path
    """

    def __init__(self, h, c, dynamics:Dynamics, mpc, env, t_init, traj, n_steps=1, max_path_length=250):
        self.horizon            =   h
        self.candidates         =   c
        self.dynamics           =   dynamics
        self.mpc                =   mpc
        self.env                =   env
        self.t_init             =   t_init
        self.trajectory         =   traj
        self.max_path_length    =   max_path_length
        self.nstack             =   dynamics.stack_n
        self.obs_flat_size      =   self.env.observation_space.shape[0]
        self.n_steps            =   n_steps


    def rollouts(self, dynamics, envs, mpc, n_rolls, min_path_length=100, max_path_length=250):
        trajectory  =   self.trajectory
        nstack  =   dynamics.stack_n
        paths   =   []
        flat_functions  =   [_env._flat_observation for _env in envs]
        if trajectory is None:
            targetposition  =   0.8 * np.ones(3, dtype=np.float32)
        else:
            targetposition  =   trajectory[0]
        
        next_target_pos =   targetposition
        

        self.env.set_targetpos(targetposition)
        obs = self.env.reset()
        obses           =   self.env.last_observation
        obses           =   [flat_fn(obses) for flat_fn in flat_functions]
        
        stack_as_list = [StackStAct(self.env.action_space.shape, self.env.observation_space.shape, n=nstack, init_st=obs) for ob in obses]
        stack_as_policy =   stack_as_list[0]
        done = False
        timestep    =   0
        cum_reward  =   0.0

        running_paths=[dict(observations=[], actions=[], rewards=[], dones=[], next_obs=[], target=[]) for _ in range(len(flat_functions))]

        while not done and timestep < max_path_length:
            
            if timestep == 120 and trajectory is None:
                next_target_pos  = np.zeros(3, dtype=np.float32)
            elif trajectory is not None:
                next_target_pos =   trajectory[timestep + 1]

            self.env.set_targetpos(next_target_pos)

            #action = mpc.get_action_PDDM(stack_as, 0.6, 5)
            action = mpc.get_action_torch(stack_as_policy)
               
            next_obs, reward, done, _   =  self.env.step(action)

            [stack_as.append(acts=action) for stack_as in stack_as_list]
            #stack_as_policy.append(acts=action)
            
            #if save_paths is not None:
            for idx, stack_as in zip(range(len(flat_functions)), stack_as_list):
                observation, action = stack_as.get()
                running_paths[idx]['observations'].append(observation.flatten())
                running_paths[idx]['actions'].append(action.flatten())
                running_paths[idx]['rewards'].append(reward)
                running_paths[idx]['dones'].append(done)
                running_paths[idx]['next_obs'].append(next_obs)
                running_paths[idx]['target'].append(targetposition)

            #if done or len(running_paths['rewards']) >= max_path_length:
            #    #print('ohhhh')
            #    paths.append(dict(
            #        observation=np.asarray(running_paths['observations']),
            #        actions=np.asarray(running_paths['actions']),
            #        rewards=np.asarray(running_paths['rewards']),
            #        dones=np.asarray(running_paths['dones']),
            #        next_obs=np.asarray(running_paths['next_obs']),
            #        target=np.asarray(running_paths['target'])
            #    ))
            
            
            targetposition  =   next_target_pos
            obses           =   self.env.last_observation

            [stack_as.append(obs=flat_functions(obses)) for flat_fn in flat_functions]

            #stack_as.append(obs=next_obs)
            cum_reward  +=  reward
            timestep += 1
        
        return running_paths




    def get_state_actions(self):
        """ Generate one rollout """
        #set_trace()
        path    =   rollouts(self.dynamics, self.env, self.mpc, 1, self.max_path_length, None, self.trajectory)
        #gt_states   =   path[0]['observation'][self.t_init:, 18*(self.nstack-1):]
        assert path[0]['rewards'].shape[0] > self.t_init + self.horizon + self.n_steps - 1, 'Too short path, try again!'
        gt_states   =   path[0]['observation'][self.t_init:self.t_init + self.horizon + self.n_steps - 1,:]
        gt_actions  =   path[0]['actions'][self.t_init:self.t_init + self.horizon + self.n_steps - 1,:]

        L = []

        for step in range(self.n_steps):
            init_stackobs    =  gt_states[step].reshape(self.nstack, -1)
            init_stackacts   =  gt_actions[step].reshape(self.nstack, -1)
            
            stack_as = StackStAct(self.env.action_space.shape, self.env.observation_space.shape, n=self.nstack)

            stack_as.fill_with_stack(init_stackobs, init_stackacts)
            
            device      =   next(self.dynamics.parameters()).device
            art_states  =   [stack_as.get_last_state()]
            art_actions =   [stack_as.get_last_action()]

            for i in range(1, self.horizon):
                obs_, acts_ =   stack_as.get()
                obs_flat    =   np.concatenate((obs_.flatten(), acts_.flatten()), axis=0)   
                obs_flat    =   self.mpc.normalize_(obs_flat)
                obs_tensor  =   torch.tensor(obs_flat, dtype=torch.float32, device=device)
                obs_tensor.unsqueeze_(0)
                next_obs    =   self.dynamics.predict_next_obs(obs_tensor, device).to('cpu')
                next_obs    =   np.asarray(next_obs.squeeze(0))
                next_action =   gt_actions[i,self.env.action_space.shape[0] * (self.nstack - 1):]
                stack_as.append(next_obs, next_action)

                art_states.append(next_obs)
                art_actions.append(next_action)

            L.append(((gt_states[step:step + self.horizon, self.obs_flat_size * (self.nstack - 1):], gt_actions[step:step+self.horizon,self.env.action_space.shape[0] * (self.nstack - 1):]), (np.stack(art_states, axis=0), np.stack(art_actions,axis=0))))
        return L

    def analize_errors(self, gt_states, ar_states):
        import matplotlib.pyplot as plt
        errors  =   np.sqrt(np.sum((gt_states-ar_states)*(gt_states-ar_states), axis=1))
        t       =   np.arange(len(errors))
        plt.plot(t, errors)
        plt.show()
    def get_errors(self, gt_states, ar_states):
        errors  =   np.sqrt(np.sum((gt_states-ar_states)*(gt_states-ar_states), axis=1))
        return errors

    def analize_pos_error(self, gt_states, ar_states):
        import matplotlib.pyplot as plt
        gt_pos  =   gt_states[:, 9:12]
        ar_pos  =   ar_states[:, 9:12]
        errors  =   np.sqrt(np.sum((gt_pos-ar_pos)*(gt_pos-ar_pos), axis=1))
        t       =   np.arange(len(errors))
        plt.plot(t, errors)
        plt.show()

if __name__ == "__main__":
    import os
    import json

    restore_folder  ='./data/sample28/'
    #save_paths_dir  =   os.path.join(restore_folder, 'rolls'+id_execution_test)
    #save_paths_dir  =   None
    with open(os.path.join(restore_folder,'config_train.json'), 'r') as fp:
        config_train    =   json.load(fp)

    config      =   {
        "env_name"          :   config_train['env_name'],
        "horizon"           :   15,
        "candidates"        :   1500,
        "discount"          :   0.99,
        "t_init"            :   30,
        "nstack"            :   config_train['nstack'],
        #"reward_type"       :   config_train['reward_type'],
        "reward_type"       :   'type1',
        "max_path_length"   :   250,
        "nrollouts"         :   20,
        "n_steps"           :   20,
        "trajectory_type"   :   'stepped',
        "sthocastic"        :   False,
        "hidden_layers"     :   config_train['hidden_layers'],
        "crippled_rotor"    :   config_train['crippled_rotor']
    }
    env_class       =   DecodeEnvironment(config['env_name'])
    env_            =   env_class(port=28001, reward_type=config['reward_type'], fault_rotor=config['crippled_rotor'])
    state_shape     =   env_.observation_space.shape
    action_shape    =   env_.action_space.shape

    device      =   torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    """ Load dynamics """
    dynamics    =   Dynamics(state_shape, action_shape, stack_n=config['nstack'], sthocastic=config['sthocastic'], hlayers=config['hidden_layers'])
    rs          =   RandomShooter(config['horizon'], config['candidates'], env_, dynamics, device, config['discount'])
    checkpoint  =   torch.load(os.path.join(restore_folder, 'params_high.pkl'))
    dynamics.load_state_dict(checkpoint['model_state_dict'])

    dynamics.mean_input =   checkpoint['mean_input']
    dynamics.std_input  =   checkpoint['std_input']
    dynamics.epsilon    =   checkpoint['epsilon']

    dynamics.to(device)
    set_trace()
    """ Send a Trajectory to follow"""
    trajectoryManager   =   Trajectory(config['max_path_length'], 2)
    trajectory          =   trajectoryManager.gen_points(config['trajectory_type']) if config['trajectory_type'] is not None else None

    scheck              =   SanityCheck(config['horizon'],config['candidates'],dynamics,rs,env_, config['t_init'], trajectory, config['n_steps'], config['max_path_length'])

    #(gt_s, gt_a), (ar_s, ar_a)  =   scheck.get_state_actions()
    L   =   scheck.get_state_actions()
    #scheck.analize_errors(gt_s,ar_s)
    #scheck.analize_pos_error(gt_s,ar_s)
    
    error_list  =   []
    for (gt_s, gt_a), (ar_s, ar_a) in L:
        errors  =   scheck.get_errors(gt_s, ar_s)
        error_list.append(errors)
    errors  =   np.vstack(error_list)

    #errors  =   np.repeat(errors, 15).reshape(15,15)
    plot_error_map(errors.T, _vmax=10.0)

    (gt_s, gt_a), (ar_s, ar_a) = L[0]

    scheck.analize_errors(gt_s, ar_s)

    (gt_s, gt_a), (ar_s, ar_a) = L[4]

    scheck.analize_errors(gt_s, ar_s)
    

    env_.close()

    print(10)